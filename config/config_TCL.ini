[general]
n_models = 5
random_seed = 42
n_epochs = 2 ; all the values below should be optimized but we fix them because of the computational cost
batch_train = 4096
learning_rate = 0.001
weight_decay = 0.01 ; L2 regularization on weights
step_size = 5 
gamma = 0.8 ;parameter in learning rate schedulers, it is a multiplicative factor by which the learning rate is reduced.
save_models = models/TCL/TCL_

[dataset_info]
dataset_train = data/dataset_data/train_dataset_10_seg_100_points.pth
dataset_val = data/dataset_data/test_dataset_10_seg_100_points.pth
dataset_test = data/dataset_data/test_dataset_10_seg_100_points.pth
n_batch_stop = 2 ;verysmall when we just wan't to check that training loop is ok
n_segment = 10
n_point = 100 
n_patient = 22000

[TCL_1]
num_layers = 1 
hidden_layers = [1]
activation = ['lrelu']
input_dim = 12 
pool_size = 1 
slope = 0.1

[TCL_2]
num_layers = 1 
hidden_layers = [3]
activation = ['lrelu']
input_dim = 12 
pool_size = 1 
slope = 0.1

[TCL_3]
num_layers = 1 
hidden_layers = [5]
activation = ['lrelu']
input_dim = 12 
pool_size = 1 
slope = 0.1

[TCL_4]
num_layers = 2 
hidden_layers = [10, 5]
activation = ['Maxout','lrelu']
input_dim = 12 
pool_size =  2
slope = 0.1

[TCL_5]
num_layers = 2 
hidden_layers = [10, 5, 3]
activation = ['ReLU', 'Maxout','lrelu']
input_dim = 12 
pool_size = 2
slope = 0.1